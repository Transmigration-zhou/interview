[toc]



## 1. 进程、线程、协程

[从 linux内核来看进程与线程的异同](https://blog.csdn.net/smilejiasmile/article/details/109389607)

### 进程与线程的区别

1. 一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程。
2. 进程是资源分配的基本单位，线程是独立调度的基本单位。
3. 进程间**不共用**变量与资源；线程间**共用**变量与资源。
4. 线程的划分尺度小于进程，使得多线程程序的并发性高。
5. 线程不能够独立执行， 必须依存在进程中。
6. 每个进程都有独立的代码和数据空间（程序上下文），程序之间的切换会有较大的开销；线程可以看做轻量级的进程，同一进程下的所有线程共享该进程的内存空间，即共享堆、全局变量、静态变量、指针、引用、打开的文件等，而独自占有栈空间，线程之间切换的开销小。
7. 进程间通信（IPC）需要借助操作系统，比较麻烦；线程间可以直接读写进程数据段来进行通信，但需要处理线程同步问题。
8. 线程存在于进程中，因此一个进程的全局变量由所有的线程共享。由于线程共享同样的系统区域，操作系统分配给一个进程的资源对该进程的所有线程都是可用的，正如全局数据可供所有线程使用一样。

#### 为什么要引入线程？

线程可以看作是轻量级的进程，但是它比进程更容易创建，也更容易撤销。（即线程成为独立调度的基本单位）

### 协程

- 协程是一种比线程更加轻量级的存在。一个线程的内存在 MB 级别，而协程只需要 KB 级别。

- 一个线程也可以拥有多个协程。

- 协程的调度完全==由用户控制==。

- **协程间切换只需保存任务的上下文，没有内核的消耗。**协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。

  ![img](https://gitee.com/Transmigration_zhou/pic/raw/master/img/20220328103921)

### 多线程vs多进程

| 对比维度       | 多进程                                                       | 多线程                                                       | 总结     |
| -------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | -------- |
| 数据共享、同步 | 数据共享复杂，需要IPC；数据是分开的，同步简单                | 因为共享进程数据，数据共享简单，但也是因为这个原因导致同步复杂 | 各有优势 |
| 内存、CPU      | 占用内存多，切换复杂，速度慢                                 | 占用内存少，切换简单，CPU利用率高                            | 线程占优 |
| 创建销毁、切换 | 创建销毁、切换复杂，速度慢                                   | 创建销毁、切换简单，速度快                                   | 线程占有 |
| 编程、调试     | 简单                                                         | 复杂                                                         | 进程占有 |
| 可靠性         | 进程间不会互相影响                                           | 一个线程挂掉将导致整个进程挂掉                               | 进程占有 |
| 分布式         | 适用于多核、多机分布式；如果一台机器不够，扩展到多台机器比较简单 | 适用于多核分布式                                             | 进程占有 |

我们来看实际应用中究竟如何判断更加合适。

1. **需要频繁创建销毁的优先用线程**

   这种原则最常见的应用就是Web服务器了，来一个连接建立一个线程，断了就销毁线程，要是用进程，创建和销毁的代价是很难承受的。

2. **需要进行大量计算的优先使用线程**

   所谓大量计算，当然就是要耗费很多CPU，切换频繁，这种情况下线程是最合适的。

   这种原则最常见的是图像处理、算法处理。

3. **强相关的处理用线程，弱相关的处理用进程**

   一般的Server需要完成如下任务：消息收发、消息处理。“消息收发”和“消息处理”就是弱相关的任务，而“消息处理”里面可能又分为“消息解码”、“业务处理”，这两个任务相对来说相关性就要强多了。因此“消息收发”和“消息处理”可以分进程设计，“消息解码”、“业务处理”可以分线程设计。

   当然这种划分方式不是一成不变的，也可以根据实际情况进行调整。

4. **可能要扩展到多机分布的用进程，多核分布的用线程**



## 2. 进程状态

![img](https://gitee.com/Transmigration_zhou/pic/raw/master/img/20220321105451.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto)

- 就绪态（ready）：拥有除过CPU之外的其他所需的所有资源。当拥有CPU时就可以转换到运行态。
- 运行态（running）：拥有CPU和所需的所有资源。
- 阻塞态（waiting 等待态）：没有CPU和所需要的资源。

### 引起进程状态转换的具体原因

运行态→阻塞态：等待使用资源；如等待外设传输；等待人工干预。

阻塞态→就绪态：资源得到满足；如外设传输结束；人工干预完成。

运行态→就绪态：运行时间片到；出现有更高优先权进程。

就绪态→运行态：CPU 空闲时选择一个就绪进程。



## 3. 进程调度算法

- **先来先服务（FCFS）**

  非抢占式的调度算法，按照请求的顺序进行调度。

  有利于长作业，但不利于短作业，因为短作业必须一直等待前面的长作业执行完毕才能执行，而长作业又需要执行很长时间，造成了短作业等待时间过长。

- **短作业优先（SJF）**

  非抢占式的调度算法，按估计运行时间最短的顺序进行调度。

  长作业有可能会饿死，处于一直等待短作业执行完毕的状态。因为如果一直有短作业到来，那么长作业永远得不到调度。

- **最短剩余时间优先（SRTN）**

  最短作业优先的抢占式版本，按剩余运行时间的顺序进行调度。 当一个新的作业到达时，其整个运行时间与当前进程的剩余时间作比较。

  如果新的进程需要的时间更少，则挂起当前进程，运行新的进程。否则新的进程等待。

- **高相应比优先（HRRN）**

  响应比 =（等待时间+要求服务时间）/要求服务时间

- **时间片轮转（RR）**

  公平地、轮流地为各个进程服务，让每个进程在一定时间隔内都可以得到响应

  按照各进程到达就绪队列的顺序，轮流让各个进程执行一个时间片。若进程未在一个时间片内执行完，则剥夺处理机，将进程重新放到就绪队列队尾重新排队。

- **优先级调度**

  按照优先级高低进行调度，有抢占式和非抢占式。

  系统进程优先级 **高于** 用户进程
  前台进程优先级 **高于** 后台进程

  操作系统更**偏好 I/O型进程**

- **多级反馈队列**

  设置多级就绪队列，各级队列==优先级从高到低==，==时间片从小到大==。

  新进程到达时先进入第1级队列，按FCFS原则排队等待被分配时间片。若用完时间片进程还未结束，则进程进入下一级队列队尾。如果此时已经在最下级的队列，则重新放回最下级队列队尾。

  可以将这种调度算法看成是时间片轮转调度算法和优先级调度算法的结合。



## 4. 进程间通信方式（IPC）

1. 管道

   通过**内核缓冲区**（可以看做一个循环队列）实现数据传输，进程以**先进先出**的方式从缓冲区存取数据。

   ![Pipe with one](https://gitee.com/Transmigration_zhou/pic/raw/master/pipe_with_one.jpg)

   1. 无名管道

      管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程之间使用。进程的亲缘关系通常是指父子进程关系。在 shell 中，我们经常使用 | 将两个命令连接起来，它的作用是把前一个命令的输出作为输入后一个命令的输入。

   2. 有名管道

      有名管道也是半双工的通信方式，但是允许在没有亲缘关系的进程之间使用。

2. 消息队列

   ![image-20220909161409746](https://gitee.com/Transmigration_zhou/pic/raw/master/image-20220909161409746.png)

   消息队列存放在内核中，可以实现消息的随机查询，消息不一定要以先进先出的次序读取，也可以按消息的类型读取。

3. 共享内存

   ![Shared Memory](https://www.tutorialspoint.com/inter_process_communication/images/shared_memory.jpg)

   系统加载一个进程的时候，分配给进程的内存并不是实际物理内存，而是虚拟内存空间。那么我们可以让两个进程各自拿出一块虚拟地址空间来，然后映射到相同的物理内存中，这样，两个进程虽然有着独立的虚拟内存空间，但有一部分却是映射到相同的物理内存，这就完成了内存共享机制了。

   使⽤共享内存的话，需要对共享的进程对共享内存的访问进⾏同步，防⽌访问对于共享数据的破坏，例如通过互斥锁或者信号量进行同步。

   共享内存的实现步骤：

   1. 创建共享内存区，通过shmget实现。在物理内存中开辟一块共享内存区。
   2. 把这块共享内存区挂接映射到两个进程的地址空间上，通过shmat实现。
   3. 完成通信之后，撤销内存映射关系，通过shmdt进行脱离。
   4. 删除共享内存区，通过shmctl实现。

   ==共享内存是最快的一种IPC方式。==因为进程可以直接读写内存，而不需要任何数据的拷贝。对于像管道和消息队列等通信方式，则需要进行四次的数据拷贝（1. 用户空间到内核；2. 内核到内存；3. 内存到内核；4. 内核到用户空间），而共享内存则只拷贝两次数据（1. 用户空间到内存；2. 内存到用户空间）。

4. 套接字 socket

   主要用于在客户端和服务器之间通过网络进行通信。

5. 信号

   用于通知接收进程某个事件已经发生，比如按下`ctrl + C`就是信号。

6. 信号量

   类似于计数器，控制多个进程多共享资源的访问。这种通信方式主要用于解决与同步相关的问题并避免竞争条件。



## 5. 进程内存空间分布

- text 代码段：存放二进制代码、字符串常量
- data 段：存放**已初始化**全局变量、静态变量、常量，且初始值不为 0
- bss 段：**未初始**化全局变量，未初始化静态变量，或者初始值为 0 的
- heap 堆区：new/malloc 手动分配的内存，需要手动释放
- stack 栈区：局部变量、函数参数，出作用域自动释放

![img](https://i.imgur.com/mFgJxBF.png)

## 6. 死锁 

### 什么是死锁?

死锁是指两个或两个以上的进程在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象。

引起死锁的原因主要有两个，一是与资源的分配策略有关，二是与并发进程的执行速度有关。	

### 死锁的条件

- 互斥条件：一段时间内某资源仅为一进程所占用。
- 请求和保持条件：已经得到了某个资源的进程可以再请求新的资源。
- 不剥夺条件：进程已获得的资源在未使用完之前，不能剥夺，只能在使用完时由自己释放。
- 循环等待条件：死锁发生时，系统中一定有两个或以上的进程组成的一条环路，该环路中每个进程都在等待着下一个进程所占有的资源。

死锁发生时，以上四个条件一定是同时满足的，如果其中任何一个条件不成立，死锁就不会发生。

### 处理方式

#### 鸵鸟策略

忽视这个问题，认为死锁不可能在系统内发生。

因为解决死锁问题的代价很高，这样做可以获得更高的性能。

大多数操作系统，处理死锁问题的办法仅仅是忽略它。

#### 死锁检测与死锁恢复

不试图阻止死锁，而是当检测到死锁发生时，采取措施进行恢复。

- 利用抢占恢复
- 利用回滚恢复
- 通过杀死进程恢复

#### 死锁预防

在程序运行之前预防发生死锁。

- **破坏互斥条件**：把独占型资源改造成共享性资源，使资源可同时访问而不是互斥使用。
- **破坏请求和保持条件**：规定所有进程在开始执行前请求所需要的全部资源。
- **破坏不剥夺条件**：当某进程获得了部分资源但得不到其他资源，则释放已占有的资源。
- **破坏循环等待条件**：给资源统一编号，进程只能按编号顺序来请求资源。

#### 死锁避免

在程序运行时避免发生死锁。

银行家算法



## 7. 一个程序从开始运行到结束的完整过程

程序要运行起来，必须经过四个步骤：预处理、编译、汇编和链接。

- 预处理：主要处理源代码文件中的以“#”开头的预编译指令，生成.i文件

  - 展开#define宏定义
  - 处理所有的条件编译指令#if
  - 插入包含的头文件\#include
  - 删除所有注释
  - 添加行号以便编译错误时能够显式行号
  - 保留#pragma编译器指令

- 编译：把预处理完的文件进行一系列词法分析、语法分析、语义分析以及优化后生成相应的汇编代码文件，生成相应的.s文件（汇编文件）

  - 词法分析：利用类似于“有限状态机”的算法，将源代码程序输入到扫描机中，将其中的字符序列分割成一系列的记号。
  - 语法分析：语法分析器对由扫描器产生的记号，进行语法分析，产生语法树。由语法分析器输出的语法树是一种以表达式为节点的树。
  - 语义分析：语法分析器只是完成了对表达式语法层面的分析，语义分析器则对表达式是否有意义进行判断，其分析的语义是静态语义。
    - 静态语义：在==编译期==就可以确定的语义。如声明与类型的匹配、类型的转换。
    - 动态语义：在==运行期==才能确定的语义。如两个整数做除法，除数为0。

  - 优化：源代码级别的一个优化过程。
  - 目标代码生成：由代码生成器将中间代码转换成目标机器代码，生成一系列的代码序列——汇编语言
    表示。
  - 目标代码优化：目标代码优化器对上述的目标机器代码进行优化：寻找合适的寻址方式、使用位移来替代乘法运算、删除多余的指令等。

  ![img](https://pic4.zhimg.com/v2-4dd909bc437680a2dad7064c4bb5e0e3_b.jpg)

- 汇编：将汇编代码转换成机器可以执行的指令，每一个汇编语句几乎都对应着一条机器指令最后生成一个.o文件（目标文件、二进制文件）

- 链接：将有关的目标文件彼此相连接，生成可执行程序

  - **1、静态链接：** 编译链接时，把库文件的代码全部加入到可执行文件中，因此生成的文件比较大，但在运行时也就不再需要库文件了。

    运行速度快：在可执行程序中已经具备了所有执行程序所需要的任何东西，在执行的时候运行速度快。

  - **2、动态链接：** 动态链接的基本思想是把程序按照模块拆分成各个相对独立部分，在程序运行时才将它们链接在一起形成一个完整的程序，而不是像静态链接一样把所有程序模块都链接成一个单独的可执行文件。

    共享库：就是即使需要每个程序都依赖同一个库，但是该库不会像静态链接那样在内存中存在多份副本，而是这多个程序在执行时共享同一份副本；

    更新方便：更新时只需要替换原来的目标文件，而无需将所有的程序再重新链接一遍。当程序下一次运行时，新版本的目标文件会被自动加载到内存并且链接起来，程序就完成了升级的目标。

    性能损耗：因为把链接推迟到了程序运行时，所以每次执行程序都需要进行链接，所以性能会有一定损失。


### 为何要链接？汇编器为何不直接输出可执行文件而是输出一个目标文件？

程序由多个模块组成，模块之间需要交互组合形成程序，模块间最常见的通信方式有模块间**函数调用和变量访问**，这需要知道目标函数和目标变量的**地址**。如果目标代码中有变量定义在其他模块或调用其他模块的函数，但每个模块都是**单独编译**，因此编译器会把这些地址搁置，链接时等链接器确定这些地址。因此编译器将源代码编译成未链接的目标文件，由链接器将目标文件链接形成可执行文件。



## 8. 虚拟地址

### 虚拟地址和物理地址的关系

虚拟内存和物理内存之间通过**页表**来**映射**

> 我们程序所使用的内存地址叫做**虚拟内存地址**
>
> 实际存在硬件里面的空间地址叫**物理内存地址**

虚拟地址分为：页号和页内偏移

<img src="https://picx.zhimg.com/v2-2aa112cea1796651a4f60902bdf2f702_720w.jpg?source=172ae18b" alt="内存管理(三)——内存分页" style="zoom: 67%;" />

### 使用虚拟地址空间的好处

1. 不同进程使用的虚拟地址彼此隔离。一个进程中的代码无法更改正在由另一进程或操作系统使用的物理内存。

   子进程执行到改变val的代码时，子进程就会发生写时拷贝。写时拷贝：顾名思义就是当代码执行到修改值的位置，子进程会拷贝一份当前相关代码通过页表映射到新的物理地址就区别于父进程中被修改变量的地址。

2. 可以缓解物理内存不足的压力，提高内存使用效率。

   当我们使用new malloc申请空间时实在虚拟内存上申请的，当你不需要使用这部分空间时，是不会给你分配物理空间的，只有当你第一次使用了这些地址才会通过内存管理算法给你开辟内存，构建页表映射关系，你才可以访问到。（延迟分配策略）



## 9. 内存管理方式

https://blog.csdn.net/Cbelieveyouself/article/details/130936291

![img](https://img-blog.csdnimg.cn/5b8fc815219f4a8abe1322fce6442114.png)

- 连续分配方式：为用户分配连续的内存空间。
  - 单一连续分配（整个用户区都给用户进程使用）
    - 优点：实现简单；无外部碎片（分配前用户进程以外的无法使用的内存空间）；不一定需要内存保护
    - 缺点：只能用于单用户、单任务OS中；有内部碎片（分配的内存内部未使用完的空间）；存储器利用率低
  - 固定分区分配（用户区分配固定大小的分区）
    - 优点：无外部碎片
    - 缺点：大程序无法装入，可能要用到覆盖，覆盖技术降低性能；内存利用率低，有内部碎片
  - 动态内存分配（根据进程大小动态分配内存）
    - 空闲分区表/链：记录各分区的分配与回收状态
    - ![img](https://img2022.cnblogs.com/blog/2207146/202205/2207146-20220502150206198-1436183617.png)
- 非连续分配管理方式（进程拆分成不同部分）
  - 页式管理机制
    - 把主存分为大小相等且固定的一页一页的形式，页较小，对比块式管理的划分力度更大，提高了内存利用率，减少了碎片。**页式管理通过页表对应虚拟地址和物理地址**。
  - 段式管理机制
    - 段式存储与页式最大区别就是：页的大小是固定的，由操作系统决定；而段的大小不固定，取决于我们当前运行的程序。
      段式：进程的地址空间按照程序自身的逻辑关系划分为若干段，每个段都有一个段名，每段从0开始。 **段式管理通过段表对应虚拟地址和物理地址**。
  - 段页式管理机制
    - **将进程按逻辑模块分段，每个段都有自己的段号，再将段分成若干大小固定的页。对内存空间的管理仍然和分页存储管理一样，将其分成若干个和页面大小相同的存储块，最后将进程的各个页分别装入各个内存块中**。
    - 段页式管理机制中由于每个段又进行了分页，所以虚拟地址结构由**段号、页号、页内地址（页内偏移量）**组成。
      - **段号的位数决定了每个进程最多可以分为几个段。**
      - **页号的位数决定了每个段最大有多少页。**
      - **页内偏移量决定了页面的大小、内存块的大小是多少。**



### 分页、分段、快表(TLB)

分页：页表记录块号。页的大小固定，系统自动生成页号，一维（只需一个符号表示地址）。

分段：段表记录段长和基址。段的大小由用户定，相当于是y=ax+b，先找到ax然后+b，二维（程序员需要段名和段内偏移）。

页是物理单位，段是逻辑单位。分页可以有效提高内存利用率，分段可以更好满足用户需求。

段页：把主存先分成若干段，每个段又分成若干页。



为了解决虚拟地址到物理地址的转换速度，操作系统在 **页表方案** 基础之上引入了 **快表** 来加速虚拟地址到物理地址的转换。我们可以把快表理解为一种特殊的高速缓冲存储器（Cache），其中的内容是页表的一部分或者全部内容。

快表命中，只需一次访存；快表未命中，需要两次访存。

快表和我们平时经常在我们开发的系统使用的缓存（比如 Redis）很像



> 分页内存管理的问题：
>
> ① 虚拟地址到物理地址的快速转换。—— 快表
>
> ② 虚拟地址空间大导致页表也很大的问题。—— 多级页表



## 10. 缓存淘汰算法（页面置换算法、缺页中断算法）

- **最佳置换算法(OPT)**

  优先淘汰最长时间内不会被访问的页面。

  缺页率最小，性能最好；但无法实现

- **先进先出置换算法(FIFO)**

  优先淘汰最先进入内存的页面。

  实现简单；但性能很差，可能出现Belady异常

  > Belady异常：如果对一个进程未分配它所要求的全部页面，有时就会出现分配的页面数增多但缺页率反而提高的异常现象

- **最近最久未使用置换算法(LRU)**

  优先淘汰最近最久没访问的页面

   性能很好；但需要硬件支持，算法开销大

- **最不常用置换算法(LFU)**

  选择「**访问次数**」最少的那个页面，并将其淘汰。

  具体实现：对每个页面设置一个「访问计数器」，每当一个页面被访问时，该页面的访问计数器就累加 1。在发生缺页中断时，淘汰计数器值最小的那个页面。

- **时钟置换算法(CLOCK)**

  循环扫描各页面 第一轮淘汰访问位=0的，并将扫描过的页面访问位改为1。若第一轮没选中，则进行第二轮扫描。

  实现简单，算法开销小；但未考虑页面是否被修改过。

- **改进型的时钟置换算法**

  **(访问位，修改位)**的形式表示各页面状态

  第一轮:从当前位置开始扫描到第一个(0, 0)的帧用于替换。本轮扫描不修改任何标识位。表示该页面最近既未被访问，又未被修改，是最佳淘汰页。

  第二轮:若第一轮扫描失败，则重新扫描，查找第一个(1, 0)的帧用于替换。本轮将所有扫描过的帧访问位设为0。表示该页面最近未被访问，但已被修改，并不是很好的淘汰页。 

  第三轮:若第二轮扫描失败，则重新扫描，查找第一个(0, 1)的帧用于替换。本轮扫描不修改任何标识位。表示该页面最近已被访问，但未被修改，该页有可能再被访问。 

  第四轮:若第三轮扫描失败，则重新扫描，查找第一个(1, 1)的帧用于替换。表示该页最近已被访问且被修改，该页可能再被访问。

  算法开销较小，性能也不错



## 11. 硬链接和软链接的区别？

硬链接是指针，所有硬链接都指向同一个磁盘块，删除一个指针并不会真正删除文件，只有把指向该文件的所有指针全部删除，才会真的删除该文件；
软链接是另外一种类型的文件，保存的是它指向文件的全路径，访问时会替换成绝对路径，软链接可以理解为Windows上的快捷方式。



## 12. 守护进程、孤儿进程与僵尸进程

- 守护进程是指==在后台运行，独立于控制终端，周期性地执行某种任务的进程==。Linux 中大多数服务器就是用守护进程的方式实现的，如 http 进程等；

- 孤儿进程是指==父进程先退出，子进程还没有退出，此时子进程就是孤儿进程==，孤儿进程会被 **init 进程（进程号为1）**收养，当孤儿进程退出时，init 进程会自动回收它们的状态；**孤儿进程不会有什么危害**

- 僵尸进程是指==子进程退出了，父进程没有退出且没有回收子进程的状态，此时子进程变为僵尸进程==，设置僵尸进程的目的是维护子进程的信息，以便父进程在以后某个时间获取。僵尸进程会一只占用进程号，因为系统所能使用的进程号是有限的，如果产生大量僵尸进程，将因没有可用的进程号而导致系统无法产生新的进程。

  如果进程不调用 wait / waitpid 的话，那么保留的那段信息就不会释放，其进程号就会一直被占用，但是系统所能使用的进程号是有限的，如果大量的产生僵死进程，将因为没有可用的进程号而导致系统不能产生新的进程。

### 如何避免僵尸进程？

- 父进程调用 wait / waitpid 等待子进程结束;
- 通过两次调用 fork，父进程首先调用一个 fork 创建一个子进程然后 waitpid 等待子进程退出，子进程再 fork一个孙进程后退出，孙进程会变成孤儿进程由 init 进程领养。



## 13. 用户态与内核态

- 用户态：用户态运行的进程可以直接读取用户程序的数据。只能受限的访问内存，且不允许访问外围设备，占用cpu的能力被剥夺，cpu资源可以被其他程序获取。

- 内核态：可以简单的理解系统态运行的进程或程序几乎可以访问计算机的任何资源，不受限制。cpu可以访问内存的所有数据，包括外围设备，例如硬盘，网卡，cpu也可以将自己从一个程序切换到另一个程序。

用户态和内核态是操作系统的两种运行级别，两者最大的区别就是特权级不同。用户态拥有最低的特权级，内核态拥有较高的特权级。运行在用户态的程序不能直接访问系统资源和执行特权指令。

**为什么要有这两态**？

需要限制不同的程序之间的访问能力，防止他们获取别的程序的内存数据，或者获取外围设备的数据，并发送到网络，CPU划分出两个权限等级：用户态和内核态。

- 核心态→用户态：执行一条特权指令，修改PSW的标志位为“用户态”，这个动作意味着操作系统将主动让出CPU使用权。
  
- 例子：刚开机的时候，CPU为内核态，操作系统的内核程序先上CPU执行。开机完成后，用户可以启动某款应用程序，此时操作系统内核程序主动让出CPU，让用户程序上CPU执行（在让出CPU之前把PSW标志位设置为用户态） 
  
- 用户态→核心态：系统调用，异常和中断。

  - 系统调用：用户进程通过系统调用申请使用操作系统提供的服务程序来完成工作，比如read()、fork()等。这是用户态进程**主动**要求切换到内核态的一种方式。系统调用的机制其核心还是使用了操作系统为用户特别开放的一个中断来实现的，例如 Linux 的 int 80h 中断。
  - 异常：当CPU在执行运行在用户态下的程序时，发生了某些事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。
  - 中断：当外围设备完成用户请求的操作后，会向CPU发送中断信号。这时CPU会暂停执行下一条指令（用户态）转而执行与该中断信号对应的中断处理程序（内核态），比如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后续操作。

  ==系统调用可以认为是用户进程「主动」发起的，异常和外围设备中断则是「被动」的。==

## 14. 异常和中断的区别

中断

![image-20221025171432519](https://gitee.com/Transmigration_zhou/pic/raw/master/image-20221025171432519.png)

异常

![image-20221025171506318](https://gitee.com/Transmigration_zhou/pic/raw/master/image-20221025171506318.png)

相同点

- 最后都是由CPU发送给内核，由内核去处理
- 处理程序的流程设计上是相似的

不同点

- 产生源不相同，异常是由CPU产生的，而中断是由硬件设备产生的
- 内核需要根据是异常还是中断调用不同的处理程序
- 中断不是时钟同步的，这意味着中断可能随时到来；异常由于是CPU产生的，所以它是时钟同步的
- 当处理中断时，处于中断上下文中；处理异常时，处于进程上下文中



## 15. 线程同步和异步

线程同步指的是线程之间“协同”，即线程之间按照规定的先后次序运行。多个线程同时访问同一资源。

线程异步后一个任务不等待前一个任务结束就可以执行，不会阻塞代码的执行。异步通过callback形式调用。多线程只是异步编程的一种实现形式。

#### 线程同步有哪些方式？

- 互斥量（互斥锁）

  只有拥有互斥对象的线程才有访问公共资源的权限，因为互斥对象只有一个，所以能保证公共资源不会同时被多个线程访问。

- 信号量

  它允许多个线程在同一时刻访问同一资源，但是需要限制在同一时刻访问此资源的最大线程数目。

  信号量允许多个线程同时进入临界区，而互斥量只允许一个线程进入临界区。

- 事件

  通过通知操作的方式来保持线程的同步，还可以方便实现对多个线程的优先级比较的操作。
  
- 读写锁（共享-独占锁）

  对某些资源的访问会出现两种情况，一种是访问的排他性，需要独占，称之为写操作；还有就是访问可以共享，称之为读操作。

- 条件变量

  条件变量用来协调想要访问共享资源的那些线程，当共享资源的状态发生变化的时候，它可以用来通知被互斥锁阻塞的线程。



## 16. 并发和并行

并发（Parallel）是指宏观上在一段时间内能同时运行多个程序

并行（Concurrent）则指同一时刻能运行多个指令

并发和并行都可以是很多个线程，就看这些线程能不能同时被多个cpu执行，如果可以就说明是并行，而并发是多个线程被一个cpu轮流切换着执行



## 17. 操作系统中堆和栈的区别

堆与栈表示两种内存管理方式

1. 管理方式不同。

   堆的内存分配取决于程序员，C/C++可以手动释放该片内存。（malloc()/free()或new/delete）

   栈的内存分配取决于编译器，用户栈在程序运行期间可以动态的扩展和收缩。

2. 空间大小不同。

   每个进程拥有的栈大小要远远小于堆大小。

   栈内存是连续的空间，堆内存一般情况不是连续的，频繁地开辟空间，释放空间容易产生内存碎片（外碎片）。

3. 分配方式不同。

   堆都是程序中由malloc()函数动态申请分配并由free()函数释放的。

   栈的分配和释放是由编译器完成的，栈的动态分配由alloca()函数完成，但是栈的动态分配和堆是不同的，他的动态分配是由编译器进行申请和释放的，无需手工实现。

4. 增长方向不同。

   堆的增长方向向上，内存地址由低到高。

   栈的增长方向向下，内存地址由高到低。

5. 存放效率不同。

   栈内存的访问直接从地址读取数据到寄存器，然后放到目标地址。

   堆内存的访问更麻烦，先将分配的地址放到寄存器，在读取地址的值，最后再放到目标文件中，开销更大。



## 18. 虚拟技术

虚拟技术指把一个物理实体转换为多个逻辑实体，主要有两种：时分复用技术和空分复用技术。

### 时分复用技术

多进程/多线程采用了时分复用技术，多个进程能在同一处理器上并发执行，让每个进程轮流占用处理器，每次只执行一小个时间片。

例子：既然一个程序需要被分配 CPU 才能正常执行，那为什么单核 CPU 的电脑中也能同时运行这么多个程序呢？

### 空分复用技术

虚拟内存使用了空分复用技术，将物理内存抽象为地址空间，每个进程都有各自的地址空间。地址空间的页被映射到物理内存，这些页并不需要全部在物理内存中，当使用到一个没有在物理内存的页时，执行页面置换算法，将该页从磁盘置换到内存中。空分复用技术的原理就是==把内存作为高速缓存来使用，只用来保存最频繁使用的部分程序，而把程序的大部分放在磁盘上==。

例子：这些程序同时运行需要的内存远大于 4 GB，那么为什么它们还可以在我的电脑上同时运行呢？



## 19. I/O模型

https://blog.csdn.net/hufi320/article/details/104411383

- 同步阻塞IO（BIO）：客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销。
  - 阻塞IO，指的是需要内核IO操作彻底完成后，才返回到用户空间，执行用户的操作。阻塞指的是用户空间程序的执行状态，用户空间程序需等到IO操作彻底完成。
  - 同步IO，是一种用户空间与内核空间的调用发起方式。同步IO是指用户空间线程是主动发起IO请求的一方，内核空间是被动接受方。异步IO则反过来，是指内核kernel是主动发起IO请求的一方，用户线程是被动接受方。
- 同步非阻塞IO（NIO）：客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。
  - 非阻塞IO，指的是用户程序不需要等待内核IO操作完成后，内核立即返回给用户一个状态值，用户空间无需等到内核的IO操作彻底完成，可以立即返回用户空间，执行用户的操作，处于非阻塞的状态。
  - 简单的说：阻塞是指用户空间（调用线程）一直在等待，而且别的事情什么都不做；非阻塞是指用户空间（调用线程）拿到状态就返回，IO操作可以干就干，不可以干，就去干的事情。
- 异步非阻塞IO（AIO）：以通知的形式来告知服务端，客户端已经将数据传输完成并且有效,服务端可以开始处理数据了。
  - 异步IO，指的是用户空间与内核空间的调用方式反过来。用户空间线程是变成被动接受的，内核空间是主动调用者。异步IO的操作基于事件和回调机制。
- IO多路复用

> https://www.cnblogs.com/linkenpark/p/12376343.html
>
> 同步/异步概念描述的是用户线程与内核的交互方式。
>
> 阻塞/非阻塞描述的是用户线程调用内核操作的方式。
>
> 拿人烧水举例来说，(人的行为好比用户程序，烧水好比内核提供的系统调用)，这两组概念翻译成大白话可以这么理解：
>
> - 同步/异步关注的是水烧开之后需不需要我来处理。
> - 阻塞/非阻塞关注的是在水烧开的这段时间是不是干了其他事。

![img](https://img2018.cnblogs.com/blog/1374212/201812/1374212-20181224100106124-1423300077.png)

BIO：适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中。

NIO：适用于连接数目多且连接比较短（轻操作）的架构，比如聊推荐服务器，并发局限于应用中。

AIO：适用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作。



## 20. read和write阻塞和非阻塞的区别

https://www.cnblogs.com/charlesblc/p/6202402.html

read：只是负责把数据从底层系统缓存copy到我们指定的位置。

write：把用户态数据copy到系统底层去，然后由系统进行发送和实际写操作。只要完成了copy，就意味着写完成。

read()和write()**系统调用在操作磁盘文件时不会直接发起磁盘访问，而是在用户缓冲区与内核缓冲区高速缓存之间复制数据**

- 阻塞读：数据不超过指定长度的时候，有多少读多少，没有数据就会一直等待。
  - 一般情况下，都需要采用循环读的方式，因为一次read不能保证读完需要的全部数据。
- 非阻塞读：有数据采用==有多少读多少==的方式，没有数据就立即返回。
  - read完一次，要判断读到的数据长度或者错误码再决定是否再次读取。注意这里的EAGAIN错误码是需要继续读取，而返回0是对方已关闭连接。
- 阻塞写：阻塞写会一直阻塞，直到所有数据都完成，再返回。
- 非阻塞写：有多少写多少。
  - 能够写多少是根据本地网络拥塞情况为标准的，当网络拥塞严重的时候，网络层没有足够的内存来进行写操作，就会出现写不完的情况。这时候，阻塞写除非被中断，都会等到数据都写完；而非阻塞写，就是能写多少算多少。



## 21. I/O多路复用

多路：指的是多个socket网络连接

复用：指的是复用一个线程、使用一个线程来检查多个文件描述符（Socket）的就绪状态

### I/O多路复用的机制

select，poll，epoll都是I/O多路复用的机制。I/O多路复用就是通过一种机制，一个进程可以监视多个描述符，一旦某个描述符就绪（一般是读就绪或者写就绪），能够通知程序进行相应的读写操作。

I/O多路复用技术的最大优势是系统开销小，系统不必创建进程/线程，也不必维护这些进程/线程，从而大大减小了系统的开销。

#### select

![img](https://pic3.zhimg.com/v2-320be0c91e2a376199b1d5eef626758e_b.gif)

select 函数作用是将 fd 的集合（bitmap）直接从用户态拷贝到内核态，由内核去遍历判断哪一个 fd 有数据。

##### 执行原理

1. 将当前进程的所有文件描述符，一次性的从用户态拷贝到内核态。
2. 在内核中快速的无差别遍历每个fd，判断是否有数据达到。
3. 将所有fd状态，从内核态拷贝到用户态，并返回已就绪fd的个数。
4. 在用户态遍历判断具体哪个fd已就绪，然后进行相应的事件处理。

##### 缺点

1. fd_set（bitmap）默认大小是1024。由FD_SETSIZE来控制，修改后性能可能会受到影响，一般该数和系统内存关系很大。
2. fd_set无法做到重用，每次循环必须重新创建。
3. 每次调用select，都需要把被监控的fd集合从用户态空间拷贝到内核态空间，高并发场景下这样的拷贝会使得消耗的资源是很大的。
4. select函数仅仅知道有几个I/O事件发生了，但并不知道具体是哪几个socket连接有I/O事件，还需要轮询去查找，时间复杂度为O(n)，处理的请求数越多，所消耗的时间越长。



#### poll

```c
struct pollfd {
    int fd; /* 文件描述符 */
    short events; /* 监听的事件 */
    short revents; /* 实际发生的事件 */
};
```

poll的工作原理本质上和select没有区别，它将用户传入的==数组==拷贝到内核空间，然后查询每个fd对应的设备状态， **但是它没有最大连接数的限制**，原因是它是基于链表来存储的。

##### 缺点

1. 仍然存在频繁的用户态到内核态拷贝。
2. 仍然需要O(n)遍历轮询。



#### epoll

epoll可以理解为event pool，不同与select、poll的轮询机制，epoll采用的是事件驱动机制，fd放在红黑树中，每个fd上有回调函数，当接收到数据时会回调该函数，同时将该fd的引用放入rdlist就绪列表中，时间复杂度为O(1)。

当调用epoll_wait检查是否有事件发生时，只需要检查event poll对象中的rdlist双链表中是否有epitem元素即可。如果rdlist不为空，则把发生的事件复制到用户态，同时将事件数量返回给用户。

epoll使用一个文件描述符管理多个描述符，将用户关系的文件描述符的事件存放到内核的一个事件表中，这样在用户空间和内核空间的copy只需一次。

epoll基于红黑树+双链表存储事件，没有最大连接数的限制。

epoll两种触发模式：

1. LT模式（Level Triggered水平触发）：该模式就是只要还有没有处理的事件就会一直通知（默认）
2. ET模式（Edge Triggered边缘触发）：该模式是当状态发生变化时才会通知

![Untitled.png](https://pic.leetcode-cn.com/1658848028-wmXtMg-Untitled.png)

#### epoll为什么高效？

- eventpoll等待队列机制，当就绪队列没有epoll事件时主动让出CPU，阻塞进程，提高CPU利用率。
- socket等待队列机制，只有接收到数据时才会将epoll事件插入就绪队列，唤醒进程获取epoll事件。
- 红黑树提高epoll事件增加，删除，修改效率。
- 任务越多，进程出让CPU概率越小，进程工作效率越高，所以epoll非常适合高并发场景。

#### epoll采用阻塞方式是否影响性能？

epoll机制本身也是阻塞的，==当epoll_wait未检测到epoll事件时，会出让CPU，阻塞进程==，这种阻塞是非常有必要的，如果不及时出让CPU会浪费CPU资源，导致其他任务无法抢占CPU，只要epoll机制能够在检测到epoll事件后，及时唤醒进程处理，并不会影响epoll性能。

#### socket采用阻塞还是非阻塞？

socket采用**非阻塞方式**。

epoll机制属于IO多路复用机制，这种机制的特点是一个进程处理多路IO请求，如果socket设置成阻塞模式会存在以下几个问题：

- 一个进程同一时间只能处理一个socket数据，如果socket被阻塞，那么该进程无法处理其他的socket数据，严重影响了性能。

阻塞的本质是进程状态和上下文的切换，频繁的阻塞会把让CPU一直处于上下文切换的状态，导致CPU瞎忙。



## 22. 线程模型

线程的模型分为三种:

- 多对一(M:1)的用户级线程模型
- 一对一(1:1)的内核级线程模型: 例如`linuxThreads`和`NPTL`
- 多对多(M:N)的两极线程模型: 例如`NGPT`